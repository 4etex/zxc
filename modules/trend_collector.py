"""
EKOSYSTEMA_FULL - –°–±–æ—Ä—â–∏–∫ —Ç—Ä–µ–Ω–¥–æ–≤
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
- YouTube RSS —Ñ–∏–¥—ã
- Google Trends
- Reddit –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø–æ—Å—Ç—ã
- Telegram –∫–∞–Ω–∞–ª—ã
"""
import asyncio
import aiohttp
import feedparser
import json
import uuid
import logging
from datetime import datetime
from typing import List, Dict, Optional
from pydantic import BaseModel
import os
from googleapiclient.discovery import build


class TrendItem(BaseModel):
    id: str
    title: str
    source: str
    url: str
    popularity_score: int
    keywords: List[str]
    timestamp: datetime
    description: str = ""
    category: str = ""

class TrendCollector:
    def __init__(self, youtube_api_key: str = None):
        self.logger = logging.getLogger(__name__)
        self.youtube_api_key = youtube_api_key
        
        # YouTube RSS —Ñ–∏–¥—ã –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        self.youtube_rss_feeds = {
            "tech": [
                "https://www.youtube.com/feeds/videos.xml?channel_id=UCBJycsmduvYEL83R_U4JriQ",  # MKBHD
                "https://www.youtube.com/feeds/videos.xml?channel_id=UCXuqSBlHAE6Xw-yeJA0Tunw",  # Linus Tech Tips
            ],
            "entertainment": [
                "https://www.youtube.com/feeds/videos.xml?channel_id=UCtxCXg-UvSnTKPOzLH4wJaQ",  # Ninja
                "https://www.youtube.com/feeds/videos.xml?channel_id=UCN7dywl5wDxTu1RM3eJ_h9Q",  # Ali-A
            ],
            "education": [
                "https://www.youtube.com/feeds/videos.xml?channel_id=UCsXVk37bltHxD1rDPwtNM8Q",  # Kurzgesagt
                "https://www.youtube.com/feeds/videos.xml?channel_id=UCsooa4yRKGN_zEE8iknghZA",  # TED-Ed
            ]
        }
        
        # Reddit –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–∞–±—Ä–µ–¥–¥–∏—Ç—ã –¥–ª—è —Ç—Ä–µ–Ω–¥–æ–≤
        self.reddit_feeds = [
            "https://www.reddit.com/r/videos/hot/.rss",
            "https://www.reddit.com/r/Documentaries/hot/.rss",
            "https://www.reddit.com/r/todayilearned/hot/.rss",
            "https://www.reddit.com/r/technology/hot/.rss",
            "https://www.reddit.com/r/science/hot/.rss"
        ]

    async def collect_youtube_trends(self) -> List[TrendItem]:
        """–°–±–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ –∏–∑ YouTube RSS —Ñ–∏–¥–æ–≤"""
        trends = []
        
        async with aiohttp.ClientSession() as session:
            for category, feeds in self.youtube_rss_feeds.items():
                for feed_url in feeds:
                    try:
                        async with session.get(feed_url) as response:
                            if response.status == 200:
                                feed_content = await response.text()
                                feed = feedparser.parse(feed_content)
                                
                                for entry in feed.entries[:5]:  # –ë–µ—Ä—ë–º —Ç–æ–ø-5 –≤–∏–¥–µ–æ
                                    trend = TrendItem(
                                        id=str(uuid.uuid4()),
                                        title=entry.title,
                                        source=f"YouTube-{category}",
                                        url=entry.link,
                                        popularity_score=self._calculate_youtube_score(entry),
                                        keywords=self._extract_keywords(entry.title),
                                        timestamp=datetime.utcnow(),
                                        description=entry.get('summary', ''),
                                        category=category
                                    )
                                    trends.append(trend)
                    except Exception as e:
                        self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ YouTube —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
        
        return trends

    async def collect_reddit_trends(self) -> List[TrendItem]:
        """–°–±–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ –∏–∑ Reddit"""
        trends = []
        
        async with aiohttp.ClientSession() as session:
            for feed_url in self.reddit_feeds:
                try:
                    async with session.get(feed_url) as response:
                        if response.status == 200:
                            feed_content = await response.text()
                            feed = feedparser.parse(feed_content)
                            
                            for entry in feed.entries[:3]:  # –ë–µ—Ä—ë–º —Ç–æ–ø-3 –ø–æ—Å—Ç–∞
                                trend = TrendItem(
                                    id=str(uuid.uuid4()),
                                    title=entry.title,
                                    source="Reddit",
                                    url=entry.link,
                                    popularity_score=self._calculate_reddit_score(entry),
                                    keywords=self._extract_keywords(entry.title),
                                    timestamp=datetime.utcnow(),
                                    description=entry.get('summary', ''),
                                    category="social"
                                )
                                trends.append(trend)
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ Reddit —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
        
        return trends

    def collect_youtube_api_trends(self) -> List[TrendItem]:
        """–°–±–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ —á–µ—Ä–µ–∑ YouTube Data API"""
        if not self.youtube_api_key:
            return []
            
        trends = []
        try:
            youtube = build('youtube', 'v3', developerKey=self.youtube_api_key)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–∏–¥–µ–æ
            request = youtube.videos().list(
                part='snippet,statistics',
                chart='mostPopular',
                regionCode='RU',
                maxResults=20
            )
            response = request.execute()
            
            for video in response.get('items', []):
                snippet = video['snippet']
                stats = video['statistics']
                
                trend = TrendItem(
                    id=str(uuid.uuid4()),
                    title=snippet['title'],
                    source="YouTube-API",
                    url=f"https://www.youtube.com/watch?v={video['id']}",
                    popularity_score=int(stats.get('viewCount', 0)) // 1000,  # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Å—á—ë—Ç
                    keywords=self._extract_keywords(snippet['title']),
                    timestamp=datetime.utcnow(),
                    description=snippet.get('description', '')[:200],
                    category=snippet.get('categoryId', 'unknown')
                )
                trends.append(trend)
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ YouTube API: {e}")
        
        return trends

    def _calculate_youtube_score(self, entry) -> int:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–¥—Å—á—ë—Ç–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è YouTube"""
        score = 50  # –ë–∞–∑–æ–≤–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (—Å–≤–µ–∂–∏–µ –≤–∏–¥–µ–æ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–µ –æ—á–∫–æ–≤)
        if hasattr(entry, 'published_parsed'):
            pub_time = datetime(*entry.published_parsed[:6])
            hours_old = (datetime.utcnow() - pub_time).total_seconds() / 3600
            if hours_old < 24:
                score += 30
            elif hours_old < 48:
                score += 15
        
        return score

    def _calculate_reddit_score(self, entry) -> int:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–¥—Å—á—ë—Ç–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è Reddit"""
        # Reddit RSS –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—á—ë—Ç—á–∏–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ª–æ–≥–∏–∫—É
        score = 40
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ "–≥–æ—Ä—è—á–∏–µ" —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        hot_words = ['viral', 'trending', 'amazing', 'incredible', 'shocking']
        for word in hot_words:
            if word.lower() in entry.title.lower():
                score += 10
                
        return score

    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ - —É–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –±–µ—Ä—ë–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
                     '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∫–∞–∫', '—á—Ç–æ', '–∏–ª–∏', '–Ω–æ'}
        
        words = text.lower().replace(',', ' ').replace('.', ' ').replace('!', ' ').replace('?', ' ').split()
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]
        
        return keywords[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤

    async def collect_all_trends(self) -> List[TrendItem]:
        """–°–±–æ—Ä –≤—Å–µ—Ö —Ç—Ä–µ–Ω–¥–æ–≤ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_trends = []
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å–æ–±–∏—Ä–∞–µ–º —Ç—Ä–µ–Ω–¥—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        youtube_trends = await self.collect_youtube_trends()
        reddit_trends = await self.collect_reddit_trends()
        api_trends = self.collect_youtube_api_trends()
        
        all_trends.extend(youtube_trends)
        all_trends.extend(reddit_trends)  
        all_trends.extend(api_trends)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        all_trends.sort(key=lambda x: x.popularity_score, reverse=True)
        
        self.logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(all_trends)} —Ç—Ä–µ–Ω–¥–æ–≤")
        return all_trends[:30]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-30 —Ç—Ä–µ–Ω–¥–æ–≤

    def get_trend_report(self, trends: List[TrendItem]) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø–æ —Ç—Ä–µ–Ω–¥–∞–º"""
        if not trends:
            return {"error": "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤"}
            
        report = {
            "total_trends": len(trends),
            "top_sources": {},
            "top_categories": {},
            "top_keywords": {},
            "timestamp": datetime.utcnow().isoformat(),
            "trends_summary": []
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for trend in trends:
            source = trend.source
            report["top_sources"][source] = report["top_sources"].get(source, 0) + 1
            
            category = trend.category
            report["top_categories"][category] = report["top_categories"].get(category, 0) + 1
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            for keyword in trend.keywords:
                report["top_keywords"][keyword] = report["top_keywords"].get(keyword, 0) + 1
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–Ω–¥–µ
            report["trends_summary"].append({
                "title": trend.title,
                "source": trend.source,
                "score": trend.popularity_score,
                "url": trend.url
            })
        
        return report


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    collector = TrendCollector(youtube_api_key="AIzaSyCuWvZcQuOG8pPgkAMPY5_QmPaql7tZUcI")
    trends = await collector.collect_all_trends()
    
    print(f"üî• –ù–∞–π–¥–µ–Ω–æ {len(trends)} –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤:")
    for i, trend in enumerate(trends[:5], 1):
        print(f"{i}. {trend.title} (Score: {trend.popularity_score})")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
    report = collector.get_trend_report(trends)
    print(f"\nüìä –û—Ç—á—ë—Ç: {report['total_trends']} —Ç—Ä–µ–Ω–¥–æ–≤ –∏–∑ {len(report['top_sources'])} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

if __name__ == "__main__":
    asyncio.run(main())